{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/westfloor22/Complete-Python-3-Bootcamp/blob/master/LELA32052_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1TkziyV4x1w"
      },
      "source": [
        "# LELA32052 Coursework Assignment\n",
        "\n",
        "This document contains instructions, guidance and code for the coursework assignment for this module.\n",
        "\n",
        "The assignment focuses on the task of intent classification. You heard about this in the lecture on dialogue systems, and read about it in this week's reading. It is an important step in modern task-based dialogue systems - given a particular piece of input from the speaker, the system tries to determine what goal the speaker is trying to achieve, in order that it can then produce an appropriate response.\n",
        "\n",
        "Your task is to build a system that takes a transcribed user utterance as input and outputs one of seven different intents:\n",
        "\n",
        "'PlayMusic', e.g. \"play easy listening\" <br>\n",
        "'AddToPlaylist' e.g. \"please add this song to road trip\" <br>\n",
        "'RateBook' e.g. \"give this novel 5 stars\"  <br>\n",
        "'SearchScreeningEvent' e.g. \"give me a list of local movie times\"  <br>\n",
        "'BookRestaurant' e.g. \"i'd like a table for four at 7pm at Asti\"   <br>\n",
        "'GetWeather' e.g. \"what's it like outside\"  <br>\n",
        "'SearchCreativeWork' \"show me the new James Bond trailer\"  <br>\n",
        "\n",
        "You are going to evaluate the performance of this system using a test set of 700 user utterances.\n",
        "\n",
        "In order to create the system you have a training set of 700 utterances and a validation/development set of 700 utterances.\n",
        "\n",
        "This notebook contains code that you can use in the development of your system. Once you have created and evaluated your system, you are going to write a report of no more than 2000 words that describes and evaluates the task, the system and the experiments.\n",
        "\n",
        "A guide as to what should go in your report can be found [here]( https://www.dropbox.com/s/zlmbk60a4ei1jdh/Writing%20your%20Computational%20Linguistics%20Research%20Report.docx)\n",
        "\n",
        "Your report should be submitted via turnitin using the link on Blackboard. You should create a PDF of your notebook and add it as an appendix to your report (not included in word count).  This is just so that I can check what you have done if it isn't clear from your report. You will not be marked on the quality of any code you might include.\n",
        "\n",
        "Please feel free to ask any questions about any part of the coursework. So that my response can benefit everyone please do so using the Coursework Discussion board on Blackboard. You can find a link to this down the left of the module Blackboard page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZTKYPQXlkwp"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9mhOx9Fo5O_"
      },
      "source": [
        "### Link drive\n",
        "\n",
        "Before you begin to build a system, there are a few steps necessary to set things up. The first of these is to link Colab to your Google Drive so that you can save files there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-FysoS88uoHa",
        "outputId": "ec1ac0a4-ab73-4a82-8471-f60f4bc2b934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "mkdir: cannot create directory ‘/content/gdrive/My Drive/Intent_Classification/’: File exists\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "!mkdir /content/gdrive/My\\ Drive/Intent_Classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBlQGSqdhYEq"
      },
      "source": [
        "### Download data and some utilities\n",
        "\n",
        "The next step is to download the data and some supporting code for the project and move it over to the Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GTM-h_dLypXc",
        "outputId": "4a03ae49-4532-4fd3-883d-b670b75d25b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 11:58:19--  https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/intent_classification_with_splits.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139084 (136K) [text/plain]\n",
            "Saving to: ‘intent_classification_with_splits.csv’\n",
            "\n",
            "intent_classificati 100%[===================>] 135.82K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-03-10 11:58:19 (3.44 MB/s) - ‘intent_classification_with_splits.csv’ saved [139084/139084]\n",
            "\n",
            "--2025-03-10 11:58:19--  https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/model.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2143 (2.1K) [application/octet-stream]\n",
            "Saving to: ‘model.pth’\n",
            "\n",
            "model.pth           100%[===================>]   2.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-10 11:58:19 (25.4 MB/s) - ‘model.pth’ saved [2143/2143]\n",
            "\n",
            "--2025-03-10 11:58:19--  https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/nn_tools.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12574 (12K) [text/plain]\n",
            "Saving to: ‘nn_tools.py’\n",
            "\n",
            "nn_tools.py         100%[===================>]  12.28K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-03-10 11:58:20 (21.3 MB/s) - ‘nn_tools.py’ saved [12574/12574]\n",
            "\n",
            "--2025-03-10 11:58:20--  https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/nn_tools2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14989 (15K) [text/plain]\n",
            "Saving to: ‘nn_tools2.py’\n",
            "\n",
            "nn_tools2.py        100%[===================>]  14.64K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-03-10 11:58:20 (11.7 MB/s) - ‘nn_tools2.py’ saved [14989/14989]\n",
            "\n",
            "--2025-03-10 11:58:20--  https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/vectorizer.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 756 [text/plain]\n",
            "Saving to: ‘vectorizer.json’\n",
            "\n",
            "vectorizer.json     100%[===================>]     756  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-10 11:58:20 (44.3 MB/s) - ‘vectorizer.json’ saved [756/756]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/intent_classification_with_splits.csv\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/nn_tools.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/nn_tools2.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/vectorizer.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omZSU3G3an8z"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "Finally we need to import some packages to use later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hGm07r91HRN0"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from nn_tools import Vocabulary, IntentVectorizer, IntentDataset, IntentClassifier\n",
        "from nn_tools2 import *\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5lUe_HTHAre"
      },
      "source": [
        "## Rule-based approach\n",
        "\n",
        "Your task here is to create a classifier using rules, in the form of regular expressions. I have provided you with the basic code for doing this. You will just need to edit the regular expressions in order to improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zJle8ZcomM5"
      },
      "source": [
        "### Loading and inspecting project data\n",
        "A valuable first step in order to understand the task is to inspect the data in order to understand the different intents being detected.\n",
        "\n",
        "First you need to load the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oFqX30mBqOeS"
      },
      "outputs": [],
      "source": [
        "intent_data=pd.read_csv('intent_classification_with_splits.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VKwiv3tzF-R"
      },
      "source": [
        "You can then examine example utterances for each of the intent types as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeWQDiVGmDrl"
      },
      "source": [
        "##### Play Music examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP0TJCpar3TM"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"PlayMusic\"][\"text\"].head(10).tolist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9MTykZ5qZo8"
      },
      "source": [
        "##### AddToPlaylist examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TXSUlg5nToK"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"AddToPlaylist\"][\"text\"].head(10).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBmwWhHXspEH"
      },
      "source": [
        "##### RateBook examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7La8pN-qm4y"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"RateBook\"][\"text\"].head(10).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbun8erusuct"
      },
      "source": [
        "##### SearchScreeningEvent examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OOlHf-QqnCh"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"SearchScreeningEvent\"][\"text\"].head(10).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiRtSmCWszDm"
      },
      "source": [
        "##### BookRestaurant examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW_cWhqDqnNN"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"BookRestaurant\"][\"text\"].head(10).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0gyT9tbs5Lc"
      },
      "source": [
        "##### GetWeather examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL_iKqhOqnWZ"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"GetWeather\"][\"text\"].head(10).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkzIGFLAs8ix"
      },
      "source": [
        "##### SearchCreativeWork examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4p6LY4cqnet"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "intent_data[intent_data[\"split\"] == \"train\"][intent_data[\"intent\"] == \"SearchCreativeWork\"][\"text\"].head(10).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCyWyTuzmCF"
      },
      "source": [
        "## Building a rule based classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4pXmXhMxSo"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "To increase the generalisability of your system you can preprocess it to, for example, convert morphological variants to a single \"lemma\". You can do this by adding different substitution (re.sub) functions to the function below. The function as is stands doesn't do anything - you need to update the re.sub statements. This isn't an essential step as you can deal with variants in your patterns, but it will help to reduce redundancy in your classifier regular expressions which you may find helpful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hN5S4nkZtg96"
      },
      "outputs": [],
      "source": [
        "def preprocess_utterance(utt):\n",
        "  utt = re.sub(\"(?<!s)s$\",\"\", utt) # Remove 's' at the end of word if not 'ss'\n",
        "  utt = re.sub(\"ed$\",\"\", utt)      # Remove past tense\n",
        "  utt = re.sub(\"ing$\",\"\", utt)     # Remove present progressive\n",
        "  utt = re.sub(\"er$\",\"\", utt)      # Remove comparative adjectives\n",
        "  utt = re.sub(\"est$\",\"\", utt)     # Remove superlative adjectives\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-haCPpLEzF-U"
      },
      "source": [
        "You can test whether you patterns are working as intented using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cN1PE_VXzF-V",
        "outputId": "82f3948c-08ce-4d7e-a4f3-c327ecb8a287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a utterance to test your preprocessing on: is it warmer?\n",
            "is it warmer?\n"
          ]
        }
      ],
      "source": [
        "test_input = input(\"Enter a utterance to test your preprocessing on: \")\n",
        "print(preprocess_utterance(test_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxiA0Zn4tMhQ"
      },
      "source": [
        "### Define patterns\n",
        "\n",
        "The function below takes an utterance as input and applies a series of regular expressions to identify the intent of the speaker. The regular expressions currently just looks for keywords taken from the intent name. You should update these patterns to be more appropriate and capture a wider range of utterances for each intent.\n",
        "\n",
        "Each time you update the code you will need to run the code cell in order to then use the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0HamWjAzV_"
      },
      "source": [
        "The assign_intent function uses the re.findall function (see week 2) in order to make as many matches as possible with each pattern. The number of matches is then counted (using the len function) for each pattern. The intent with the largest number of matches is then returned as the predicted intent. Imagine for example that your patterns for PlayMusic and GetWeather were as follows: <br>\n",
        "PlayMusic_Pattern = re.compile(\"play|music\") <br>\n",
        "GetWeather_Pattern = re.compile(\"get|weather\") <br>\n",
        "while the input utterances was \"play some music by the weather girls\".\n",
        "In this case the PlayMusic pattern would match twice (for play and music) while the GetWeather pattern would only match once. PlayMusic would be returned as the predicted intent. Where there is a tie (as would happen if, for example, the input was simply \"play the weather girls\") a prediction is randomly sampled from among the tied intents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GTaIRW-x4Ezb"
      },
      "outputs": [],
      "source": [
        "def assign_intent(utt, verbose=False):\n",
        "  PlayMusic_Pattern = re.compile(\"play|music|by|something|hear|fm|spotify|youtube|zvooq\")\n",
        "  AddToPlaylist_Pattern = re.compile(\"add|playlist|song|album|artist\")\n",
        "  RateBook_Pattern = re.compile(\"rate|book\")\n",
        "  SearchScreeningEvent_Pattern = re.compile(r\"screen|cinema|theatre|movie|film|show|time|schedule|nearest|close|theatre|playing\")\n",
        "  BookRestaurant_Pattern = re.compile(\"book|restaurant\")\n",
        "  GetWeather_Pattern = re.compile(\"get|weather\")\n",
        "  SearchCreativeWork_Pattern = re.compile(\"creative\")\n",
        "\n",
        "  weights = {}\n",
        "  weights['PlayMusic'] = len(re.findall(PlayMusic_Pattern,  utt))\n",
        "  weights['AddToPlaylist'] = len(re.findall(AddToPlaylist_Pattern,  utt))\n",
        "  weights['RateBook'] = len(re.findall(RateBook_Pattern,  utt))\n",
        "  weights['SearchScreeningEvent'] = len(re.findall(SearchScreeningEvent_Pattern,  utt))\n",
        "  weights['BookRestaurant'] = len(re.findall(BookRestaurant_Pattern,  utt))\n",
        "  weights['GetWeather'] = len(re.findall(GetWeather_Pattern,  utt))\n",
        "  weights['SearchCreativeWork'] = len(re.findall(SearchCreativeWork_Pattern,  utt))\n",
        "  if verbose:\n",
        "      print(weights)\n",
        "  if max(weights.values()) == 0:\n",
        "      return random.choice(list(weights.keys()))\n",
        "  else:\n",
        "      weights_as_list = list(weights.items())\n",
        "      random.shuffle(weights_as_list)\n",
        "      weights=dict(weights_as_list)\n",
        "      return max(weights, key=lambda key: weights[key])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_20_words(intent_data, intent_class):\n",
        "    # Filter the data for the validation set and PlayMusic intent\n",
        "    filtered_data = intent_data[(intent_data['split'] == 'val') & (intent_data['intent'] == intent_class)]\n",
        "\n",
        "    # Combine all the text into a single string\n",
        "    all_text = \" \".join(filtered_data['text'])\n",
        "\n",
        "    # Tokenize the text (split into words) using a regular expression\n",
        "    words = re.findall(r'\\b\\w+\\b', all_text.lower())  # Extract words and convert to lowercase\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Get the 20 most common words\n",
        "    top_20_words = word_counts.most_common(20)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Top 20 Words:\")\n",
        "    for word, count in top_20_words:\n",
        "        print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "yYsjNmhe7_uy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_top_20_words(intent_data, \"AddToPlaylist\")"
      ],
      "metadata": {
        "id": "8r0GRSSm8BEN",
        "outputId": "6c16d026-6737-4f72-81df-9579c847aced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Words:\n",
            "to: 93\n",
            "add: 89\n",
            "playlist: 81\n",
            "the: 71\n",
            "my: 51\n",
            "this: 23\n",
            "song: 15\n",
            "album: 12\n",
            "artist: 12\n",
            "track: 11\n",
            "onto: 11\n",
            "of: 10\n",
            "a: 10\n",
            "tune: 9\n",
            "is: 9\n",
            "put: 8\n",
            "you: 8\n",
            "i: 7\n",
            "s: 7\n",
            "in: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cwzJHqDLokL"
      },
      "source": [
        "### Evaluation\n",
        "When you run this cell you will be asked to enter an utterance. When you press return the scores for each classification of your input will be printed. You can use this to check whether your preprocess_utterance and/or assign_intent functions are working as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZqyRr_sIJvw"
      },
      "outputs": [],
      "source": [
        "new_input = input(\"Enter a utterance to classify: \")\n",
        "prediction = assign_intent(preprocess_utterance(new_input),verbose=True)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsec0kYpLh0s"
      },
      "source": [
        "In order to perform a stricter assessment of the performance of your classifier, you should examine its performance on the validation dataset. If you run the cell below you will be told the accuracy score on those 700 utterances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pCDozgVm2XnZ",
        "outputId": "a8fe2760-bcf9-45d8-bb4f-2564e626b87f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5257142857142857\n"
          ]
        }
      ],
      "source": [
        "predicted = [assign_intent(preprocess_utterance(item)) for item in intent_data[intent_data['split'] == \"val\"]['text']]\n",
        "true = intent_data[intent_data['split'] == \"val\"]['intent']\n",
        "accuracy = (predicted == true).sum()/len(true)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xajHmMX33-oC"
      },
      "source": [
        "Running the next cell will give you a \"confusion matrix\". This tells you the number of times that each intent as given in the rows is classified (correctly or otherwise) as each intent as represented by the columns. The columns are displayed as numbers but you can check which each of these numbers stands for by looking into the parentheses after each intent in the rows.\n",
        "\n",
        "Studying this will give you an idea of where the classifier might be going wrong, and therefore of how you might update your patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "j1t-etZ3L7iS",
        "outputId": "82c93fe5-5c5a-48c1-cce1-a16a6504970c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           0   1   2   3   4   5   6\n",
            "AddToPlaylist (0)         86   1   7   0   6  11  12\n",
            "BookRestaurant (1)         0  51   9   0   9  14   9\n",
            "GetWeather (2)             0   1  55   1   8  17  12\n",
            "PlayMusic (3)             11   2  10  98  10  17  38\n",
            "RateBook (4)               2  39   9   0  56  17   9\n",
            "SearchCreativeWork (5)     0   4   5   0   8  17  15\n",
            "SearchScreeningEvent (6)   1   2   5   1   3   7   5\n"
          ]
        }
      ],
      "source": [
        "print(pd.DataFrame(confusion_matrix(predicted, true), index=[\"AddToPlaylist (0)\", \"BookRestaurant (1)\", \"GetWeather (2)\", \"PlayMusic (3)\", \"RateBook (4)\", \"SearchCreativeWork (5)\", \"SearchScreeningEvent (6)\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV9Py45v2HdG"
      },
      "source": [
        "Once you are happy that you have defined the best patterns that you can, you should evaluate the performance of your classifier on the test data (a set of 700 utterances that you haven't looked at). The accuracy printed is what you should include in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3DqHjOs02yk"
      },
      "outputs": [],
      "source": [
        "predicted = [assign_intent(preprocess_utterance(item)) for item in intent_data[intent_data['split'] == \"test\"]['text']]\n",
        "true = intent_data[intent_data['split'] == \"test\"]['intent']\n",
        "accuracy = (predicted == true).sum()/len(true)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvH8ufhD5fzd"
      },
      "source": [
        "You can also generate a confusion matrix for the test data and use this in the discussion of the results in your write up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODBNlsOQ5fEf"
      },
      "outputs": [],
      "source": [
        "print(pd.DataFrame(confusion_matrix(predicted, true), index=[\"AddToPlaylist (0)\", \"BookRestaurant (1)\", \"GetWeather (2)\", \"PlayMusic (3)\", \"RateBook (4)\", \"SearchCreativeWork (5)\", \"SearchScreeningEvent (6)\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbiBTPm2EWM_"
      },
      "source": [
        "## Single Layer Perceptron Classifier\n",
        "\n",
        "The next classifier you can build is single-layer perceptron. The specification of this model is in the next cell. You shouldn't make any changes to this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhqmeFunSxTY"
      },
      "outputs": [],
      "source": [
        "class IntentClassifierPerceptron(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features (int): the size of the input feature vector\n",
        "        \"\"\"\n",
        "        super(IntentClassifierPerceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=True):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, num_features)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,)\n",
        "        \"\"\"\n",
        "\n",
        "        y_out = self.fc1(x_in)\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out,dim=1)\n",
        "        return y_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxIBFz42QIF2"
      },
      "source": [
        "### Preprocessing\n",
        "As with the rule-based model, to increase the generalisability of your system you can preprocess the text to, for example, convert morphological variants to a single \"lemma\". You can do ths by adding different substitution (re.sub) functions to the function below. The function as is stands doesn't do anything - you need to update the re.sub statements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJK3bcpeQIF-"
      },
      "outputs": [],
      "source": [
        "def preprocess_utterance(utt):\n",
        "  utt = re.sub(\"\",\"\", utt)\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d7BwY4CQIF-"
      },
      "source": [
        "For this machine-learning-based model we are going to make the preprocessing changes to the data used, and this will involve making changes to the file we have saved. When we need to revert to the original unaltered file, we can then run the following cell."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DabzSQ_pc8dj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr2Uiq-YQIF_"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/intent_classification_with_splits.csv -O intent_classification_with_splits.csv\n",
        "!cp intent_classification_with_splits.csv /content/gdrive/My\\ Drive/Intent_Classification/\n",
        "intent_data=pd.read_csv('intent_classification_with_splits.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lltyd5emQIF_"
      },
      "source": [
        "You can check that your preprocessing patterns are doing what we want by testing them on examples using the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmmw5G5QQIF_"
      },
      "outputs": [],
      "source": [
        "test_input = input(\"Enter a utterance to test your preprocessing on: \")\n",
        "print(preprocess_utterance(test_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKxW9JpRQIF_"
      },
      "source": [
        "Once you are happy with your preprocessing you can then transform the text in the training, validation and test data using the following cell. This alters the data on the disk so if you want to undo any changes later you will have to revert to the original data (as described above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLg0h1JIQIF_"
      },
      "outputs": [],
      "source": [
        "intent_data=pd.read_csv('intent_classification_with_splits.csv')\n",
        "intent_data['text'] = [preprocess_utterance(item) for item in intent_data['text']]\n",
        "intent_data.to_csv('intent_classification_with_splits.csv', index=False)\n",
        "!cp intent_classification_with_splits.csv /content/gdrive/My\\ Drive/Intent_Classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkTtqltm6Ctc"
      },
      "source": [
        "### Training the model\n",
        "In order to first initialise your model and then train it, you should run the following cell. The training will take a minute or two to complete - the progress bars will tell you how far along it is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj-qJML1ja7t"
      },
      "outputs": [],
      "source": [
        "params = initialise()\n",
        "classifier = IntentClassifierPerceptron(input_dim=len(params.vectorizer.text_vocab),output_dim=len(params.vectorizer.intent_vocab))\n",
        "train_state = trainModel(params, params.dataset, classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejmO5vhuHn72"
      },
      "source": [
        "### Test on individual utterance\n",
        "\n",
        "As with the rule-based classifier you can look at the performance of the system on a single example utterance. This can be used to see whether your preprocessing is helping to capture the cases you hoped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OEZKHmDKG2M"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "new_utterance = input(\"Enter a utterance to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_intent(preprocess_utterance(new_utterance), classifier, params.vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_utterance,\n",
        "                                    prediction['intent'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulIyMBO1qGfZ"
      },
      "source": [
        "### Evaluate performance on validation data\n",
        "In order to evaluate the performance of your system while you tweak your preprocessing you can evaluate performance on the validation data, and look at a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZjsKUtLldPL"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "predicted, true = evaluate(params, classifier, train_state, 'val')\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['val_acc']))\n",
        "print(pd.DataFrame(confusion_matrix(predicted, true), index=[\"AddToPlaylist\", \"BookRestaurant\", \"GetWeather\", \"PlayMusic\", \"RateBook\", \"SearchCreativeWork\", \"SearchScreeningEvent\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGm1SE5ZqMis"
      },
      "source": [
        "### Evaluate performance on test data\n",
        "\n",
        "Once you are happy that model performance is as good as you can achieve with this model type you should evaluate its accuracy on the test data. You can also use the confusion matrix for error analysis in your write up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF4udyBOqDgy"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "predicted, true = evaluate(params, classifier, train_state, 'test')\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n",
        "print(pd.DataFrame(confusion_matrix(predicted, true), index=[\"AddToPlaylist\", \"BookRestaurant\", \"GetWeather\", \"PlayMusic\", \"RateBook\", \"SearchCreativeWork\", \"SearchScreeningEvent\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmJWrOFBvwJf"
      },
      "source": [
        "## Multilayer neural network classifier\n",
        "\n",
        "The third kind of classifier that you should build is a two-layer neural network. The model is defined below. Again you don't need to change this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUfDaievThXH"
      },
      "outputs": [],
      "source": [
        "class IntentClassifierMLP(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features (int): the size of the input feature vector\n",
        "        \"\"\"\n",
        "        super(IntentClassifierMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=True):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, num_features)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,)\n",
        "        \"\"\"\n",
        "        intermediate_vector = F.relu(self.fc1(x_in))\n",
        "        prediction_vector = self.fc2(intermediate_vector)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector,dim=1)\n",
        "        return prediction_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZCtR8k29BV6"
      },
      "source": [
        "### Preprocessing\n",
        "As with the previous models, to increase the generalisability of your system you can preprocess the text to, for example, convert morphological variants to a single \"lemma\". You can do ths by adding different substitution (re.sub) functions to the function below. The function as is stands doesn't do anything - you need to update the re.sub statements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzvODPtSF9up"
      },
      "outputs": [],
      "source": [
        "def preprocess_utterance(utt):\n",
        "  utt = re.sub(\"\",\"\", utt)\n",
        "  utt = re.sub(\"\",\"\", utt)\n",
        "  utt = re.sub(\"\",\"\", utt)\n",
        "  return utt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYR060QAdXF"
      },
      "source": [
        "For this machine-learning-based model we are going to make the preprocessing changes to the data use, and this will involve making changes to the file we have saved. When we need to revert to the original unaltered file, we can then run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaMZzsyvAJyZ"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/Intent_Classification/intent_classification_with_splits.csv -O intent_classification_with_splits.csv\n",
        "!cp intent_classification_with_splits.csv /content/gdrive/My\\ Drive/Intent_Classification/\n",
        "intent_data=pd.read_csv('intent_classification_with_splits.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KCskx_pBPn1"
      },
      "source": [
        "You can check that your preprocessing patterns are doing what we want by testing them on examples using the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLqEVhjxBWsX"
      },
      "outputs": [],
      "source": [
        "test_input = input(\"Enter a utterance to test your preprocessing on: \")\n",
        "print(preprocess_utterance(test_input))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpfyaMF1A8ra"
      },
      "source": [
        "Once you are happy with your preprocessing you can then transform the text in the training, validation and test data using the following cell. This alters the data on the disk so if you want to undo any changes later you will have to revert to the original data (as described above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrK4_EBB9bf7"
      },
      "outputs": [],
      "source": [
        "intent_data=pd.read_csv('intent_classification_with_splits.csv')\n",
        "intent_data['text'] = [preprocess_utterance(item) for item in intent_data['text']]\n",
        "intent_data.to_csv('intent_classification_with_splits.csv', index=False)\n",
        "!cp intent_classification_with_splits.csv /content/gdrive/My\\ Drive/Intent_Classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUYKpA_k7j8B"
      },
      "source": [
        "### Training model\n",
        "You can now move on to training the model using your preprocessed data.\n",
        "\n",
        "One important variable that you can change is the number of nodes to include in your hidden layer. You can set this parameter in the cell below. The default is 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2IqgaF072Wl"
      },
      "outputs": [],
      "source": [
        "n_hidden_dims = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYEM37XU8BT3"
      },
      "source": [
        "You can then initialise and train the model by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XN0GKp9eoNY"
      },
      "outputs": [],
      "source": [
        "params = initialise()\n",
        "classifier = IntentClassifierMLP(input_dim=len(params.vectorizer.text_vocab),hidden_dim=n_hidden_dims,output_dim=len(params.vectorizer.intent_vocab))\n",
        "train_state = trainModel(params, params.dataset, classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwwWsWk8oks"
      },
      "source": [
        "The following cell allows you to look at the performance of the system on a single example utterance. This can be used to see whether your preprocessing (and/or your choice of hidden dimensions) is capturing the cases you hoped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lED2pnPpLDpj"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "new_utterance = input(\"Enter a utterance to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_intent(preprocess_utterance(new_utterance), classifier, params.vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_utterance,\n",
        "                                    prediction['intent'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9tEyVp9qv2F"
      },
      "source": [
        "### Evaluate model on validation data\n",
        "\n",
        "In order to evaluate the performance of your system while you tweak it you can evaluate performance on the validation data, and look at a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6y-UHQVquba"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "predicted, true = evaluate(params, classifier, train_state, 'val')\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['val_acc']))\n",
        "print(pd.DataFrame(confusion_matrix(predicted, true), index=[\"AddToPlaylist\", \"BookRestaurant\", \"GetWeather\", \"PlayMusic\", \"RateBook\", \"SearchCreativeWork\", \"SearchScreeningEvent\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WQeZtjvq1gc"
      },
      "source": [
        "### Evaluate model on test data\n",
        "\n",
        "Once you are happy that model performance is as good as you can achieve with this model type you should evaluate its accuracy on the test data. You can also use the confusion matrix for error analysis in your write up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-EvHcvIqu6y"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "predicted, true = evaluate(params, classifier, train_state, 'test')\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n",
        "print(pd.DataFrame(confusion_matrix(predicted, true), index=[\"AddToPlaylist\", \"BookRestaurant\", \"GetWeather\", \"PlayMusic\", \"RateBook\", \"SearchCreativeWork\", \"SearchScreeningEvent\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uivLXlpkzF-c"
      },
      "source": [
        "## That's it!\n",
        "\n",
        "Once you have run through all of this notebook, made all of the additions and changes you want, and run all of the experiments you want you should write up the results following the guidance [here]( https://www.dropbox.com/s/zlmbk60a4ei1jdh/Writing%20your%20Computational%20Linguistics%20Research%20Report.docx)\n",
        "\n",
        "Your report should be submitted via turnitin using the link on Blackboard.\n",
        "\n",
        "To repeat a couple of things from the top of the sheet:\n",
        "\n",
        "- Once you have completed your work in this notebook please create a PDF including all of your changes and include it in your submission. This is just so that I can check what you have done if it isn't clear from your report. You will not be marked on the quality of any code you might include.\n",
        "\n",
        "- Please feel free to ask any questions about any part of the coursework. So that my response can benefit everyone please do so using the Coursework Discussion board on Blackboard. You can find a link to this down the left of the module Blackboard page.\n",
        "\n",
        "I hope you find it an enjoyable and worthwhile exercise.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}